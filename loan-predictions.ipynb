{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-10-18T05:07:56.564815Z","iopub.execute_input":"2022-10-18T05:07:56.565240Z","iopub.status.idle":"2022-10-18T05:07:56.578896Z","shell.execute_reply.started":"2022-10-18T05:07:56.565209Z","shell.execute_reply":"2022-10-18T05:07:56.577735Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df=pd.read_csv(\"../input/loan-prediction-problem-dataset/train_u6lujuX_CVtuZ9i.csv\")# reading the file ","metadata":{"execution":{"iopub.status.busy":"2022-10-18T05:07:56.594829Z","iopub.execute_input":"2022-10-18T05:07:56.595234Z","iopub.status.idle":"2022-10-18T05:07:56.605177Z","shell.execute_reply.started":"2022-10-18T05:07:56.595199Z","shell.execute_reply":"2022-10-18T05:07:56.603744Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head() # Finding the head","metadata":{"execution":{"iopub.status.busy":"2022-10-18T05:07:56.607354Z","iopub.execute_input":"2022-10-18T05:07:56.608279Z","iopub.status.idle":"2022-10-18T05:07:56.629230Z","shell.execute_reply.started":"2022-10-18T05:07:56.608242Z","shell.execute_reply":"2022-10-18T05:07:56.628106Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.isnull() # False means there is no null , true means yes there is null values ","metadata":{"execution":{"iopub.status.busy":"2022-10-18T05:07:56.631091Z","iopub.execute_input":"2022-10-18T05:07:56.632062Z","iopub.status.idle":"2022-10-18T05:07:56.659380Z","shell.execute_reply.started":"2022-10-18T05:07:56.632026Z","shell.execute_reply":"2022-10-18T05:07:56.658058Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(df.shape) # Shape of the data row=614 whereas columns is 13","metadata":{"execution":{"iopub.status.busy":"2022-10-18T05:07:56.661448Z","iopub.execute_input":"2022-10-18T05:07:56.661937Z","iopub.status.idle":"2022-10-18T05:07:56.669357Z","shell.execute_reply.started":"2022-10-18T05:07:56.661889Z","shell.execute_reply":"2022-10-18T05:07:56.667920Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"execution":{"iopub.status.busy":"2022-10-18T05:07:56.672233Z","iopub.execute_input":"2022-10-18T05:07:56.673481Z","iopub.status.idle":"2022-10-18T05:07:56.690135Z","shell.execute_reply.started":"2022-10-18T05:07:56.673443Z","shell.execute_reply":"2022-10-18T05:07:56.688202Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.isnull().sum() # to find the null values ","metadata":{"execution":{"iopub.status.busy":"2022-10-18T05:07:56.691920Z","iopub.execute_input":"2022-10-18T05:07:56.692907Z","iopub.status.idle":"2022-10-18T05:07:56.703655Z","shell.execute_reply.started":"2022-10-18T05:07:56.692856Z","shell.execute_reply":"2022-10-18T05:07:56.702362Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head() # Again check the data , even this step is not required. You can skip this step ","metadata":{"execution":{"iopub.status.busy":"2022-10-18T05:07:56.705347Z","iopub.execute_input":"2022-10-18T05:07:56.706265Z","iopub.status.idle":"2022-10-18T05:07:56.730476Z","shell.execute_reply.started":"2022-10-18T05:07:56.706219Z","shell.execute_reply":"2022-10-18T05:07:56.729266Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.duplicated().any() # Find the duplicate values we have or not ( here there is no duplicate values) ","metadata":{"execution":{"iopub.status.busy":"2022-10-18T05:07:56.732326Z","iopub.execute_input":"2022-10-18T05:07:56.733075Z","iopub.status.idle":"2022-10-18T05:07:56.744627Z","shell.execute_reply.started":"2022-10-18T05:07:56.733030Z","shell.execute_reply":"2022-10-18T05:07:56.743355Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Now we will find the status percentage of loan \nplt.figure(figsize=(8,6))\nsns.countplot(df['Loan_Status']);\n\nprint('The percentage of Y class : %.2f'%(df['Loan_Status'].value_counts()[0] / len(df)))\nprint('The percentage of N class : %.2f'%(df['Loan_Status'].value_counts()[1] / len(df)))","metadata":{"execution":{"iopub.status.busy":"2022-10-18T05:07:56.747637Z","iopub.execute_input":"2022-10-18T05:07:56.748672Z","iopub.status.idle":"2022-10-18T05:07:56.959933Z","shell.execute_reply.started":"2022-10-18T05:07:56.748620Z","shell.execute_reply":"2022-10-18T05:07:56.958684Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Now we will find the columns name to understand the data \ndf.columns","metadata":{"execution":{"iopub.status.busy":"2022-10-18T05:07:56.961604Z","iopub.execute_input":"2022-10-18T05:07:56.962037Z","iopub.status.idle":"2022-10-18T05:07:56.969393Z","shell.execute_reply.started":"2022-10-18T05:07:56.962002Z","shell.execute_reply":"2022-10-18T05:07:56.968029Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.drop('Loan_ID',axis=1, inplace=True) \n#axis = 1 means we are dealing with column\n# When inplace = True is used, it performs operation on data and \n#nothing is returned. When inplace=False is used, it performs operation \n#on data and returns a new copy of data","metadata":{"execution":{"iopub.status.busy":"2022-10-18T05:07:56.971082Z","iopub.execute_input":"2022-10-18T05:07:56.971398Z","iopub.status.idle":"2022-10-18T05:07:56.981080Z","shell.execute_reply.started":"2022-10-18T05:07:56.971370Z","shell.execute_reply":"2022-10-18T05:07:56.980182Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2022-10-18T05:07:56.982560Z","iopub.execute_input":"2022-10-18T05:07:56.983285Z","iopub.status.idle":"2022-10-18T05:07:57.008085Z","shell.execute_reply.started":"2022-10-18T05:07:56.983249Z","shell.execute_reply":"2022-10-18T05:07:57.006893Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head(1)","metadata":{"execution":{"iopub.status.busy":"2022-10-18T05:07:57.009935Z","iopub.execute_input":"2022-10-18T05:07:57.010286Z","iopub.status.idle":"2022-10-18T05:07:57.029577Z","shell.execute_reply.started":"2022-10-18T05:07:57.010249Z","shell.execute_reply":"2022-10-18T05:07:57.028261Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"grid=sns.FacetGrid(df,col=\"Loan_Status\",size=3.2,aspect=1.6)\ngrid.map(sns.countplot,'Credit_History');\n\n# In the below figure dataset  we can say we did not give to the loan person having o credit history \n# But give to loan person having 1 credit history \n# Moreover we can simply say , we most of the people get loan whom do they have credit history 1 and most of \n#Most of the people didn't get loan those do have 0 credit history.","metadata":{"execution":{"iopub.status.busy":"2022-10-18T05:07:57.030833Z","iopub.execute_input":"2022-10-18T05:07:57.031550Z","iopub.status.idle":"2022-10-18T05:07:57.440046Z","shell.execute_reply.started":"2022-10-18T05:07:57.031512Z","shell.execute_reply":"2022-10-18T05:07:57.438796Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ngrid = sns.FacetGrid(df,col='Loan_Status', size=3.2, aspect=1.6)\ngrid.map(sns.countplot,'Gender');\n# Here most the male and female got the loan , gender feature is not important","metadata":{"execution":{"iopub.status.busy":"2022-10-18T05:07:57.441688Z","iopub.execute_input":"2022-10-18T05:07:57.442121Z","iopub.status.idle":"2022-10-18T05:07:57.845153Z","shell.execute_reply.started":"2022-10-18T05:07:57.442075Z","shell.execute_reply":"2022-10-18T05:07:57.843371Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(df[\"Gender\"]) # get to know what is number of gender ","metadata":{"execution":{"iopub.status.busy":"2022-10-18T05:07:57.846699Z","iopub.execute_input":"2022-10-18T05:07:57.847161Z","iopub.status.idle":"2022-10-18T05:07:57.856535Z","shell.execute_reply.started":"2022-10-18T05:07:57.847122Z","shell.execute_reply":"2022-10-18T05:07:57.855125Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Checking the people with married status get loan or not \nplt.figure(figsize=(8,6))\nsns.countplot(x=\"Married\", hue=\"Loan_Status\",data=df);\n\n\n# We can see in the plot ,those persons got married have higher chances to get loan\n# This feature is important\n","metadata":{"execution":{"iopub.status.busy":"2022-10-18T05:07:57.857706Z","iopub.execute_input":"2022-10-18T05:07:57.858068Z","iopub.status.idle":"2022-10-18T05:07:58.084333Z","shell.execute_reply.started":"2022-10-18T05:07:57.858036Z","shell.execute_reply":"2022-10-18T05:07:58.083061Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head(1)","metadata":{"execution":{"iopub.status.busy":"2022-10-18T05:07:58.089211Z","iopub.execute_input":"2022-10-18T05:07:58.089576Z","iopub.status.idle":"2022-10-18T05:07:58.107869Z","shell.execute_reply.started":"2022-10-18T05:07:58.089544Z","shell.execute_reply":"2022-10-18T05:07:58.106530Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(8,6))\nsns.countplot(x=\"Self_Employed\",hue=\"Loan_Status\",data=df)","metadata":{"execution":{"iopub.status.busy":"2022-10-18T05:07:58.109271Z","iopub.execute_input":"2022-10-18T05:07:58.109718Z","iopub.status.idle":"2022-10-18T05:07:58.354295Z","shell.execute_reply.started":"2022-10-18T05:07:58.109685Z","shell.execute_reply":"2022-10-18T05:07:58.353054Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"grid=sns.FacetGrid(df,col=\"Self_Employed\",size =3.2, aspect=1.6)\ngrid.map(sns.countplot,'Loan_Status');\n# Self employed people get less loan , we can canclude from graph","metadata":{"execution":{"iopub.status.busy":"2022-10-18T05:07:58.356218Z","iopub.execute_input":"2022-10-18T05:07:58.356617Z","iopub.status.idle":"2022-10-18T05:07:58.756718Z","shell.execute_reply.started":"2022-10-18T05:07:58.356576Z","shell.execute_reply":"2022-10-18T05:07:58.755514Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head(1)","metadata":{"execution":{"iopub.status.busy":"2022-10-18T05:07:58.760682Z","iopub.execute_input":"2022-10-18T05:07:58.761068Z","iopub.status.idle":"2022-10-18T05:07:58.781858Z","shell.execute_reply.started":"2022-10-18T05:07:58.761035Z","shell.execute_reply":"2022-10-18T05:07:58.781043Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Based on the property area , we will analyze from the figure Urban areas having high loan status,where as rural area have high chances of getting loan , semiurban areas rejection of getting loan have higher chances.\nplt.figure(figsize=(15,5))\nsns.countplot(x=\"Property_Area\", hue=\"Loan_Status\",data=df);","metadata":{"execution":{"iopub.status.busy":"2022-10-18T05:07:58.783556Z","iopub.execute_input":"2022-10-18T05:07:58.784309Z","iopub.status.idle":"2022-10-18T05:07:59.045139Z","shell.execute_reply.started":"2022-10-18T05:07:58.784259Z","shell.execute_reply":"2022-10-18T05:07:59.043510Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Now we will find the relationship between Applicant income \n# As there is no relationship between applicantincome and loan get \nplt.scatter(df['ApplicantIncome'], df['Loan_Status']);\n","metadata":{"execution":{"iopub.status.busy":"2022-10-18T05:07:59.047309Z","iopub.execute_input":"2022-10-18T05:07:59.047843Z","iopub.status.idle":"2022-10-18T05:07:59.253867Z","shell.execute_reply.started":"2022-10-18T05:07:59.047752Z","shell.execute_reply":"2022-10-18T05:07:59.252721Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Next step to find the median of the data ,we are taking median because only through ...\n# this outiler can be reomved \ndf.groupby(\"Loan_Status\").median()","metadata":{"execution":{"iopub.status.busy":"2022-10-18T05:07:59.256000Z","iopub.execute_input":"2022-10-18T05:07:59.256344Z","iopub.status.idle":"2022-10-18T05:07:59.276274Z","shell.execute_reply.started":"2022-10-18T05:07:59.256314Z","shell.execute_reply":"2022-10-18T05:07:59.275129Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Processing the data before  writing the algorithm. ","metadata":{}},{"cell_type":"markdown","source":"### Find the missing value ","metadata":{}},{"cell_type":"code","source":"df.isnull().sum() # simple way to find the null values","metadata":{"execution":{"iopub.status.busy":"2022-10-18T05:07:59.277852Z","iopub.execute_input":"2022-10-18T05:07:59.278663Z","iopub.status.idle":"2022-10-18T05:07:59.289583Z","shell.execute_reply.started":"2022-10-18T05:07:59.278615Z","shell.execute_reply":"2022-10-18T05:07:59.288319Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.isnull().sum().sort_values(ascending=False) # then finding in the descending way ","metadata":{"execution":{"iopub.status.busy":"2022-10-18T05:07:59.291534Z","iopub.execute_input":"2022-10-18T05:07:59.292285Z","iopub.status.idle":"2022-10-18T05:07:59.305850Z","shell.execute_reply.started":"2022-10-18T05:07:59.292236Z","shell.execute_reply":"2022-10-18T05:07:59.304716Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Seperate the numerical and categorical data ","metadata":{}},{"cell_type":"code","source":"cat_data = []\nnum_data = []\n\nfor i,c in enumerate(df.dtypes):\n    if c == object:\n        cat_data.append(df.iloc[:, i])\n    else :\n        num_data.append(df.iloc[:, i])\n        ","metadata":{"execution":{"iopub.status.busy":"2022-10-18T05:07:59.307266Z","iopub.execute_input":"2022-10-18T05:07:59.308238Z","iopub.status.idle":"2022-10-18T05:07:59.316816Z","shell.execute_reply.started":"2022-10-18T05:07:59.308193Z","shell.execute_reply":"2022-10-18T05:07:59.316017Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cat_data = pd.DataFrame(cat_data).transpose()\nnum_data = pd.DataFrame(num_data).transpose()","metadata":{"execution":{"iopub.status.busy":"2022-10-18T05:07:59.318105Z","iopub.execute_input":"2022-10-18T05:07:59.318763Z","iopub.status.idle":"2022-10-18T05:07:59.360104Z","shell.execute_reply.started":"2022-10-18T05:07:59.318720Z","shell.execute_reply":"2022-10-18T05:07:59.359098Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cat_data.head()","metadata":{"execution":{"iopub.status.busy":"2022-10-18T05:07:59.361623Z","iopub.execute_input":"2022-10-18T05:07:59.361939Z","iopub.status.idle":"2022-10-18T05:07:59.374509Z","shell.execute_reply.started":"2022-10-18T05:07:59.361911Z","shell.execute_reply":"2022-10-18T05:07:59.373681Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_data.head()","metadata":{"execution":{"iopub.status.busy":"2022-10-18T05:07:59.375591Z","iopub.execute_input":"2022-10-18T05:07:59.376531Z","iopub.status.idle":"2022-10-18T05:07:59.396252Z","shell.execute_reply.started":"2022-10-18T05:07:59.376493Z","shell.execute_reply":"2022-10-18T05:07:59.394974Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# cat_data\n# If you want to fill every column with its own most frequent value you can use\n\ncat_data = cat_data.apply(lambda x:x.fillna(x.value_counts().index[0]))\ncat_data.isnull().sum().any() # no more missing data ","metadata":{"execution":{"iopub.status.busy":"2022-10-18T05:07:59.397410Z","iopub.execute_input":"2022-10-18T05:07:59.397712Z","iopub.status.idle":"2022-10-18T05:07:59.417899Z","shell.execute_reply.started":"2022-10-18T05:07:59.397685Z","shell.execute_reply":"2022-10-18T05:07:59.416746Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# num_data\n# fill every missing value with their previous value in the same column\n# if th data is missing , it could be filled  with the previous value of the same column\n\nnum_data.fillna(method='bfill', inplace=True)\nnum_data.isnull().sum().any() # no more missing data","metadata":{"execution":{"iopub.status.busy":"2022-10-18T05:07:59.419277Z","iopub.execute_input":"2022-10-18T05:07:59.420417Z","iopub.status.idle":"2022-10-18T05:07:59.430980Z","shell.execute_reply.started":"2022-10-18T05:07:59.420371Z","shell.execute_reply":"2022-10-18T05:07:59.429839Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Now we will make the value does not have numerical , so we label those.\n### Label Encoder ","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder \nle = LabelEncoder()\ncat_data.head()","metadata":{"execution":{"iopub.status.busy":"2022-10-18T05:07:59.432262Z","iopub.execute_input":"2022-10-18T05:07:59.433279Z","iopub.status.idle":"2022-10-18T05:07:59.454435Z","shell.execute_reply.started":"2022-10-18T05:07:59.433235Z","shell.execute_reply":"2022-10-18T05:07:59.453030Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Next step to change the target column \ntarget_values = {'Y':0,'N':1}\ntarget=cat_data['Loan_Status']\ncat_data.drop('Loan_Status',axis=1,inplace=True)\ntarget=target.map(target_values)","metadata":{"execution":{"iopub.status.busy":"2022-10-18T05:07:59.456072Z","iopub.execute_input":"2022-10-18T05:07:59.456449Z","iopub.status.idle":"2022-10-18T05:07:59.464752Z","shell.execute_reply.started":"2022-10-18T05:07:59.456415Z","shell.execute_reply":"2022-10-18T05:07:59.463492Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Trasform other categories \nfor i in cat_data:\n    cat_data[i]=le.fit_transform(cat_data[i])","metadata":{"execution":{"iopub.status.busy":"2022-10-18T05:07:59.466113Z","iopub.execute_input":"2022-10-18T05:07:59.466697Z","iopub.status.idle":"2022-10-18T05:07:59.479816Z","shell.execute_reply.started":"2022-10-18T05:07:59.466664Z","shell.execute_reply":"2022-10-18T05:07:59.478323Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"target.head()","metadata":{"execution":{"iopub.status.busy":"2022-10-18T05:07:59.481765Z","iopub.execute_input":"2022-10-18T05:07:59.482246Z","iopub.status.idle":"2022-10-18T05:07:59.493030Z","shell.execute_reply.started":"2022-10-18T05:07:59.482208Z","shell.execute_reply":"2022-10-18T05:07:59.491664Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cat_data.head()","metadata":{"execution":{"iopub.status.busy":"2022-10-18T05:07:59.494301Z","iopub.execute_input":"2022-10-18T05:07:59.494802Z","iopub.status.idle":"2022-10-18T05:07:59.510656Z","shell.execute_reply.started":"2022-10-18T05:07:59.494770Z","shell.execute_reply":"2022-10-18T05:07:59.509154Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df=pd.concat([cat_data, num_data,target], axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-10-18T05:07:59.512416Z","iopub.execute_input":"2022-10-18T05:07:59.513124Z","iopub.status.idle":"2022-10-18T05:07:59.519531Z","shell.execute_reply.started":"2022-10-18T05:07:59.513087Z","shell.execute_reply":"2022-10-18T05:07:59.518445Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2022-10-18T05:07:59.520991Z","iopub.execute_input":"2022-10-18T05:07:59.521636Z","iopub.status.idle":"2022-10-18T05:07:59.547352Z","shell.execute_reply.started":"2022-10-18T05:07:59.521600Z","shell.execute_reply":"2022-10-18T05:07:59.546357Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Train the data \n##### Move forward to know the train the data.\n##### We are going to use stratifiedshuffleSplit ","metadata":{}},{"cell_type":"code","source":"X=pd.concat([cat_data,num_data],axis=1)\ny=target","metadata":{"execution":{"iopub.status.busy":"2022-10-18T05:07:59.548762Z","iopub.execute_input":"2022-10-18T05:07:59.549147Z","iopub.status.idle":"2022-10-18T05:07:59.555333Z","shell.execute_reply.started":"2022-10-18T05:07:59.549105Z","shell.execute_reply":"2022-10-18T05:07:59.554194Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# we will use StratifiedShuffleSplit to split the data Taking into consideration that we will get the same ratio on the target column\n\nfrom sklearn.model_selection import StratifiedShuffleSplit\n\nsss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n\nfor train, test in sss.split(X, y):\n    X_train, X_test = X.iloc[train], X.iloc[test]\n    y_train, y_test = y.iloc[train], y.iloc[test]\n    \nprint('X_train shape', X_train.shape)\nprint('y_train shape', y_train.shape)\nprint('X_test shape', X_test.shape)\nprint('y_test shape', y_test.shape)\n\n# almost same ratio\nprint('\\nratio of target in y_train :',y_train.value_counts().values/ len(y_train))\nprint('ratio of target in y_test :',y_test.value_counts().values/ len(y_test))\nprint('ratio of target in original_data :',df['Loan_Status'].value_counts().values/ len(df))","metadata":{"execution":{"iopub.status.busy":"2022-10-18T05:07:59.556874Z","iopub.execute_input":"2022-10-18T05:07:59.557758Z","iopub.status.idle":"2022-10-18T05:07:59.574950Z","shell.execute_reply.started":"2022-10-18T05:07:59.557712Z","shell.execute_reply":"2022-10-18T05:07:59.574031Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# we will use 4 different models for training\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\n\nmodels = {\n    'LogisticRegression': LogisticRegression(random_state=42),\n    'KNeighborsClassifier': KNeighborsClassifier(),\n    'SVC': SVC(random_state=42),\n    'DecisionTreeClassifier': DecisionTreeClassifier(max_depth=1, random_state=42)\n}\n\n# we can see that best model is LogisticRegression at least for now, SVC is just memorizing the data so it is overfitting .","metadata":{"execution":{"iopub.status.busy":"2022-10-18T05:07:59.576613Z","iopub.execute_input":"2022-10-18T05:07:59.577004Z","iopub.status.idle":"2022-10-18T05:07:59.584204Z","shell.execute_reply.started":"2022-10-18T05:07:59.576944Z","shell.execute_reply":"2022-10-18T05:07:59.583175Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"build functions\nwe are going to build 3 functions :\n1) loss : to evaluate our models\n\nprecision\nrecall\nf1\nlog_loss\naccuracy_score\n2) train_eval_train : to evaluate our models in the same data that we train it on .\n\n3) train_eval_cross : to evaluate our models using different data that we train the model on .\n\nStratifiedKFold\nso you may ask why we don't just train our model and evaluate it without building this functions ?\nactually you can do that,but mostly your model will not work good at beginning, so you need to change something about your data to improve your accuracy , by changing i mean data processing, and every step you will make, you should evaluate your model to see if it is improving or not, so to not do this step every time, this functions will make life easy as you go :)","metadata":{}},{"cell_type":"code","source":"# loss \nfrom sklearn.metrics import precision_score, recall_score, f1_score, log_loss, accuracy_score\n\ndef loss(y_true, y_pred, retu=False):\n    pre = precision_score(y_true, y_pred)\n    rec = recall_score(y_true, y_pred)\n    f1 = f1_score(y_true, y_pred)\n    loss = log_loss(y_true, y_pred)\n    acc = accuracy_score(y_true, y_pred)\n    \n    if retu:\n        return pre, rec, f1, loss, acc\n    else:\n        print('  pre: %.3f\\n  rec: %.3f\\n  f1: %.3f\\n  loss: %.3f\\n  acc: %.3f' % (pre, rec, f1, loss, acc))","metadata":{"execution":{"iopub.status.busy":"2022-10-18T05:07:59.586015Z","iopub.execute_input":"2022-10-18T05:07:59.586388Z","iopub.status.idle":"2022-10-18T05:07:59.602724Z","shell.execute_reply.started":"2022-10-18T05:07:59.586355Z","shell.execute_reply":"2022-10-18T05:07:59.601781Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train_eval_train\n\ndef train_eval_train(models, X, y):\n    for name, model in models.items():\n        print(name,':')\n        model.fit(X, y)\n        loss(y, model.predict(X))\n        print('-'*30)\n        \ntrain_eval_train(models, X_train, y_train)\n\n# we can see that best model is LogisticRegression at least for now, SVC is just memorizing the data so it is overfitting .","metadata":{"execution":{"iopub.status.busy":"2022-10-18T05:07:59.608489Z","iopub.execute_input":"2022-10-18T05:07:59.609593Z","iopub.status.idle":"2022-10-18T05:07:59.717196Z","shell.execute_reply.started":"2022-10-18T05:07:59.609534Z","shell.execute_reply":"2022-10-18T05:07:59.715968Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train.shape","metadata":{"execution":{"iopub.status.busy":"2022-10-18T05:07:59.718600Z","iopub.execute_input":"2022-10-18T05:07:59.719053Z","iopub.status.idle":"2022-10-18T05:07:59.725522Z","shell.execute_reply.started":"2022-10-18T05:07:59.719020Z","shell.execute_reply":"2022-10-18T05:07:59.724490Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train_eval_cross\n# in the next cell i will be explaining this function\n\nfrom sklearn.model_selection import StratifiedKFold\nskf = StratifiedKFold(n_splits=10, random_state=42, shuffle=True)\n\ndef train_eval_cross(models, X, y, folds):\n    # we will change X & y to dataframe because we will use iloc (iloc don't work on numpy array)\n    X = pd.DataFrame(X) \n    y = pd.DataFrame(y)\n    idx = [' pre', ' rec', ' f1', ' loss', ' acc']\n    for name, model in models.items():\n        ls = []\n        print(name,':')\n\n        for train, test in folds.split(X, y):\n            model.fit(X.iloc[train], y.iloc[train]) \n            y_pred = model.predict(X.iloc[test]) \n            ls.append(loss(y.iloc[test], y_pred, retu=True))\n        print(pd.DataFrame(np.array(ls).mean(axis=0), index=idx)[0])  #[0] because we don't want to show the name of the column\n        print('-'*30)\n        \ntrain_eval_cross(models, X_train, y_train, skf)\n\n# ohhh, as i said SVC is just memorizing the data, and you can see that here DecisionTreeClassifier is better than LogisticRegression","metadata":{"execution":{"iopub.status.busy":"2022-10-18T05:07:59.727329Z","iopub.execute_input":"2022-10-18T05:07:59.728222Z","iopub.status.idle":"2022-10-18T05:08:00.554600Z","shell.execute_reply.started":"2022-10-18T05:07:59.728185Z","shell.execute_reply":"2022-10-18T05:08:00.553474Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# some explanation of the above function\n\nx = []\nidx = [' pre', ' rec', ' f1', ' loss', ' acc']\n\n# we will use one model\nlog = LogisticRegression()\n\nfor train, test in skf.split(X_train, y_train):\n    log.fit(X_train.iloc[train], y_train.iloc[train])\n    ls = loss(y_train.iloc[test], log.predict(X_train.iloc[test]), retu=True)\n    x.append(ls)\n    \n# thats what we get\npd.DataFrame(x, columns=idx)\n\n# (column 0 represent the precision_score of the 10 folds)\n# (row 0 represent the (pre, rec, f1, loss, acc) for the first fold)\n# then we should find the mean of every column\n# pd.DataFrame(x, columns=idx).mean(axis=0)","metadata":{"execution":{"iopub.status.busy":"2022-10-18T05:08:00.555909Z","iopub.execute_input":"2022-10-18T05:08:00.556247Z","iopub.status.idle":"2022-10-18T05:08:00.901366Z","shell.execute_reply.started":"2022-10-18T05:08:00.556216Z","shell.execute_reply":"2022-10-18T05:08:00.900146Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Next Step to improve the model \n#### Feature engineering ","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"# ooh, we got it right for most of the features, as you can see we've say at the first of the kernel ,\n# that Credit_Histroy and Married etc, are good features, actually Credit_Histroy is the best .\n\ndata_corr = pd.concat([X_train, y_train], axis=1)\ncorr = data_corr.corr()\nplt.figure(figsize=(10,7))\nsns.heatmap(corr, annot=True);\n\n# here we got 58% similarity between LoanAmount & ApplicantIncome \n# and that may be bad for our model so we will see what we can do\n","metadata":{"execution":{"iopub.status.busy":"2022-10-18T05:08:00.902859Z","iopub.execute_input":"2022-10-18T05:08:00.903206Z","iopub.status.idle":"2022-10-18T05:08:02.025427Z","shell.execute_reply.started":"2022-10-18T05:08:00.903174Z","shell.execute_reply":"2022-10-18T05:08:02.023996Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train.head()","metadata":{"execution":{"iopub.status.busy":"2022-10-18T05:08:02.027600Z","iopub.execute_input":"2022-10-18T05:08:02.029627Z","iopub.status.idle":"2022-10-18T05:08:02.049180Z","shell.execute_reply.started":"2022-10-18T05:08:02.029574Z","shell.execute_reply":"2022-10-18T05:08:02.047860Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# I will try to make some operations on some features, here I just tried diffrent operations on diffrent features,\n# having experience in the field, and having knowledge about the data will also help\n\nX_train['new_col'] = X_train['CoapplicantIncome'] / X_train['ApplicantIncome']  \nX_train['new_col_2'] = X_train['LoanAmount'] * X_train['Loan_Amount_Term'] ","metadata":{"execution":{"iopub.status.busy":"2022-10-18T05:08:02.050495Z","iopub.execute_input":"2022-10-18T05:08:02.050859Z","iopub.status.idle":"2022-10-18T05:08:02.065697Z","shell.execute_reply.started":"2022-10-18T05:08:02.050826Z","shell.execute_reply":"2022-10-18T05:08:02.064366Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_corr = pd.concat([X_train, y_train], axis=1)\ncorr = data_corr.corr()\nplt.figure(figsize=(10,7))\nsns.heatmap(corr, annot=True);\n\n# new_col 0.03 , new_col_2, 0.047\n# not that much , but that will help us reduce the number of features","metadata":{"execution":{"iopub.status.busy":"2022-10-18T05:08:38.637058Z","iopub.execute_input":"2022-10-18T05:08:38.637486Z","iopub.status.idle":"2022-10-18T05:08:39.959395Z","shell.execute_reply.started":"2022-10-18T05:08:38.637445Z","shell.execute_reply":"2022-10-18T05:08:39.958250Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train.drop(['CoapplicantIncome', 'ApplicantIncome', 'Loan_Amount_Term', 'LoanAmount'], axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-10-18T05:08:55.830074Z","iopub.execute_input":"2022-10-18T05:08:55.830472Z","iopub.status.idle":"2022-10-18T05:08:55.838616Z","shell.execute_reply.started":"2022-10-18T05:08:55.830440Z","shell.execute_reply":"2022-10-18T05:08:55.837355Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_eval_cross(models, X_train, y_train, skf)\n\n# ok, SVC is improving, but LogisticRegression is overfitting\n# i wan't change nothing so we can see what will happen as we go","metadata":{"execution":{"iopub.status.busy":"2022-10-18T05:09:12.557978Z","iopub.execute_input":"2022-10-18T05:09:12.558391Z","iopub.status.idle":"2022-10-18T05:09:13.138459Z","shell.execute_reply.started":"2022-10-18T05:09:12.558354Z","shell.execute_reply":"2022-10-18T05:09:13.137341Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# first lets take a look at the value counts of every label\n\nfor i in range(X_train.shape[1]):\n    print(X_train.iloc[:,i].value_counts(), end='\\n------------------------------------------------\\n')\n","metadata":{"execution":{"iopub.status.busy":"2022-10-18T05:09:48.739774Z","iopub.execute_input":"2022-10-18T05:09:48.740233Z","iopub.status.idle":"2022-10-18T05:09:48.759374Z","shell.execute_reply.started":"2022-10-18T05:09:48.740193Z","shell.execute_reply":"2022-10-18T05:09:48.757905Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### We will work on the features that have varied values ","metadata":{}},{"cell_type":"code","source":"# new_col_2\n\n# we can see we got right_skewed\n# we can solve this problem with very simple statistical teqniq , by taking the logarithm of all the values\n# because when data is normally distributed that will help improving our model\n\nfrom scipy.stats import norm\n\nfig, ax = plt.subplots(1,2,figsize=(20,5))\n\nsns.distplot(X_train['new_col_2'], ax=ax[0], fit=norm)\nax[0].set_title('new_col_2 before log')\n\nX_train['new_col_2'] = np.log(X_train['new_col_2'])  # logarithm of all the values\n\nsns.distplot(X_train['new_col_2'], ax=ax[1], fit=norm)\nax[1].set_title('new_col_2 after log');","metadata":{"execution":{"iopub.status.busy":"2022-10-18T05:11:10.643679Z","iopub.execute_input":"2022-10-18T05:11:10.644159Z","iopub.status.idle":"2022-10-18T05:11:11.182547Z","shell.execute_reply.started":"2022-10-18T05:11:10.644112Z","shell.execute_reply":"2022-10-18T05:11:11.181281Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# now we will evaluate our models, and i will do that continuously ,so i don't need to mention that every time\n\ntrain_eval_cross(models, X_train, y_train, skf)\n\n# wooow our models improved really good by just doing the previous step","metadata":{"execution":{"iopub.status.busy":"2022-10-18T05:12:07.724329Z","iopub.execute_input":"2022-10-18T05:12:07.725313Z","iopub.status.idle":"2022-10-18T05:12:08.453991Z","shell.execute_reply.started":"2022-10-18T05:12:07.725270Z","shell.execute_reply":"2022-10-18T05:12:08.452733Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# new_col\n\n# most of our data is 0 , so we will try to change other values to 1\n\nprint('before:')\nprint(X_train['new_col'].value_counts())\n\nX_train['new_col'] = [x if x==0 else 1 for x in X_train['new_col']]\nprint('-'*50)\nprint('\\nafter:')\nprint(X_train['new_col'].value_counts())","metadata":{"execution":{"iopub.status.busy":"2022-10-18T05:12:36.080157Z","iopub.execute_input":"2022-10-18T05:12:36.080535Z","iopub.status.idle":"2022-10-18T05:12:36.092534Z","shell.execute_reply.started":"2022-10-18T05:12:36.080503Z","shell.execute_reply":"2022-10-18T05:12:36.091158Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_eval_cross(models, X_train, y_train, skf)\n\n# ok we are improving our models as we go","metadata":{"execution":{"iopub.status.busy":"2022-10-18T05:12:57.448461Z","iopub.execute_input":"2022-10-18T05:12:57.448937Z","iopub.status.idle":"2022-10-18T05:12:58.179614Z","shell.execute_reply.started":"2022-10-18T05:12:57.448899Z","shell.execute_reply":"2022-10-18T05:12:58.178207Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in range(X_train.shape[1]):\n    print(X_train.iloc[:,i].value_counts(), end='\\n------------------------------------------------\\n')\n    \n# looks better","metadata":{"execution":{"iopub.status.busy":"2022-10-18T05:13:18.650386Z","iopub.execute_input":"2022-10-18T05:13:18.650763Z","iopub.status.idle":"2022-10-18T05:13:18.669034Z","shell.execute_reply.started":"2022-10-18T05:13:18.650732Z","shell.execute_reply":"2022-10-18T05:13:18.667498Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Outliers\n #### there is different techniques to handle outliers, here we are going to use \n","metadata":{}},{"cell_type":"code","source":"# we will use boxplot to detect outliers\n\nsns.boxplot(X_train['new_col_2']);\nplt.title('new_col_2 outliers', fontsize=15);\nplt.xlabel('');","metadata":{"execution":{"iopub.status.busy":"2022-10-18T05:14:39.479648Z","iopub.execute_input":"2022-10-18T05:14:39.480230Z","iopub.status.idle":"2022-10-18T05:14:39.657609Z","shell.execute_reply.started":"2022-10-18T05:14:39.480190Z","shell.execute_reply":"2022-10-18T05:14:39.656504Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"threshold = 1.5  # this number is hyper parameter , as much as you reduce it, as much as you remove more points\n                 # you can just try different values the deafult value is (1.5) it works good for most cases\n                 # but be careful, you don't want to try a small number because you may loss some important information from the data .\n                 \n            \nnew_col_2_out = X_train['new_col_2']\nq25, q75 = np.percentile(new_col_2_out, 25), np.percentile(new_col_2_out, 75) # Q25, Q75\nprint('Quartile 25: {} , Quartile 75: {}'.format(q25, q75))\n\niqr = q75 - q25\nprint('iqr: {}'.format(iqr))\n\ncut = iqr * threshold\nlower, upper = q25 - cut, q75 + cut\nprint('Cut Off: {}'.format(cut))\nprint('Lower: {}'.format(lower))\nprint('Upper: {}'.format(upper))\n\noutliers = [x for x in new_col_2_out if x < lower or x > upper]\nprint('Nubers of Outliers: {}'.format(len(outliers)))\nprint('outliers:{}'.format(outliers))\n\ndata_outliers = pd.concat([X_train, y_train], axis=1)\nprint('\\nlen X_train before dropping the outliers', len(data_outliers))\ndata_outliers = data_outliers.drop(data_outliers[(data_outliers['new_col_2'] > upper) | (data_outliers['new_col_2'] < lower)].index)\n\nprint('len X_train before dropping the outliers', len(data_outliers))","metadata":{"execution":{"iopub.status.busy":"2022-10-18T05:15:44.695190Z","iopub.execute_input":"2022-10-18T05:15:44.695615Z","iopub.status.idle":"2022-10-18T05:15:44.713678Z","shell.execute_reply.started":"2022-10-18T05:15:44.695580Z","shell.execute_reply":"2022-10-18T05:15:44.712269Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train = data_outliers.drop('Loan_Status', axis=1)\ny_train = data_outliers['Loan_Status']","metadata":{"execution":{"iopub.status.busy":"2022-10-18T05:16:04.379928Z","iopub.execute_input":"2022-10-18T05:16:04.380313Z","iopub.status.idle":"2022-10-18T05:16:04.386496Z","shell.execute_reply.started":"2022-10-18T05:16:04.380281Z","shell.execute_reply":"2022-10-18T05:16:04.385595Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.boxplot(X_train['new_col_2']);\nplt.title('new_col_2 without outliers', fontsize=15);\nplt.xlabel('');\n\n# good :)","metadata":{"execution":{"iopub.status.busy":"2022-10-18T05:16:21.285203Z","iopub.execute_input":"2022-10-18T05:16:21.285609Z","iopub.status.idle":"2022-10-18T05:16:21.463280Z","shell.execute_reply.started":"2022-10-18T05:16:21.285574Z","shell.execute_reply":"2022-10-18T05:16:21.461805Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_eval_cross(models, X_train, y_train, skf)","metadata":{"execution":{"iopub.status.busy":"2022-10-18T05:16:35.371663Z","iopub.execute_input":"2022-10-18T05:16:35.372087Z","iopub.status.idle":"2022-10-18T05:16:36.100837Z","shell.execute_reply.started":"2022-10-18T05:16:35.372050Z","shell.execute_reply":"2022-10-18T05:16:36.099351Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### features selection","metadata":{}},{"cell_type":"code","source":"# Self_Employed got really bad corr (-0.00061) , let's try remove it and see what will happen\n\ndata_corr = pd.concat([X_train, y_train], axis=1)\ncorr = data_corr.corr()\nplt.figure(figsize=(10,7))\nsns.heatmap(corr, annot=True);","metadata":{"execution":{"iopub.status.busy":"2022-10-18T05:17:32.311755Z","iopub.execute_input":"2022-10-18T05:17:32.312198Z","iopub.status.idle":"2022-10-18T05:17:33.582477Z","shell.execute_reply.started":"2022-10-18T05:17:32.312159Z","shell.execute_reply":"2022-10-18T05:17:33.581242Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#X_train.drop(['Self_Employed'], axis=1, inplace=True)\n\ntrain_eval_cross(models, X_train, y_train, skf)\n\n# looks like Self_Employed is not important\n# KNeighborsClassifier improved\n\n# droping all the features Except for Credit_History actually improved KNeighborsClassifier and didn't change anything in other models\n# so you can try it by you self\n# but don't forget to do that on testing data too\n\n#X_train.drop(['Self_Employed','Dependents', 'new_col_2', 'Education', 'Gender', 'Property_Area','Married', 'new_col'], axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-10-18T05:17:58.929549Z","iopub.execute_input":"2022-10-18T05:17:58.930149Z","iopub.status.idle":"2022-10-18T05:17:59.674462Z","shell.execute_reply.started":"2022-10-18T05:17:58.930115Z","shell.execute_reply":"2022-10-18T05:17:59.673382Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_corr = pd.concat([X_train, y_train], axis=1)\ncorr = data_corr.corr()\nplt.figure(figsize=(10,7))\nsns.heatmap(corr, annot=True);","metadata":{"execution":{"iopub.status.busy":"2022-10-18T05:18:27.807291Z","iopub.execute_input":"2022-10-18T05:18:27.807737Z","iopub.status.idle":"2022-10-18T05:18:28.678152Z","shell.execute_reply.started":"2022-10-18T05:18:27.807664Z","shell.execute_reply":"2022-10-18T05:18:28.677017Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"### Evalaute the model on test_data","metadata":{}},{"cell_type":"code","source":"X_test.head()","metadata":{"execution":{"iopub.status.busy":"2022-10-18T05:19:25.426631Z","iopub.execute_input":"2022-10-18T05:19:25.427665Z","iopub.status.idle":"2022-10-18T05:19:25.446193Z","shell.execute_reply.started":"2022-10-18T05:19:25.427626Z","shell.execute_reply":"2022-10-18T05:19:25.444977Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_test_new = X_test.copy()","metadata":{"execution":{"iopub.status.busy":"2022-10-18T05:19:41.294077Z","iopub.execute_input":"2022-10-18T05:19:41.294476Z","iopub.status.idle":"2022-10-18T05:19:41.300140Z","shell.execute_reply.started":"2022-10-18T05:19:41.294442Z","shell.execute_reply":"2022-10-18T05:19:41.298871Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x = []\n\nX_test_new['new_col'] = X_test_new['CoapplicantIncome'] / X_test_new['ApplicantIncome']  \nX_test_new['new_col_2'] = X_test_new['LoanAmount'] * X_test_new['Loan_Amount_Term']\nX_test_new.drop(['CoapplicantIncome', 'ApplicantIncome', 'Loan_Amount_Term', 'LoanAmount'], axis=1, inplace=True)\n\nX_test_new['new_col_2'] = np.log(X_test_new['new_col_2'])\n\nX_test_new['new_col'] = [x if x==0 else 1 for x in X_test_new['new_col']]\n\n#X_test_new.drop(['Self_Employed'], axis=1, inplace=True)\n\n# drop all the features Except for Credit_History\n#X_test_new.drop(['Self_Employed','Dependents', 'new_col_2', 'Education', 'Gender', 'Property_Area','Married', 'new_col'], axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-10-18T05:20:02.529619Z","iopub.execute_input":"2022-10-18T05:20:02.530066Z","iopub.status.idle":"2022-10-18T05:20:02.541983Z","shell.execute_reply.started":"2022-10-18T05:20:02.530032Z","shell.execute_reply":"2022-10-18T05:20:02.540640Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_test_new.head()\n","metadata":{"execution":{"iopub.status.busy":"2022-10-18T05:20:19.302623Z","iopub.execute_input":"2022-10-18T05:20:19.303943Z","iopub.status.idle":"2022-10-18T05:20:19.318133Z","shell.execute_reply.started":"2022-10-18T05:20:19.303900Z","shell.execute_reply":"2022-10-18T05:20:19.317268Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for name,model in models.items():\n    print(name, end=':\\n')\n    loss(y_test, model.predict(X_test_new))\n    print('-'*40)","metadata":{"execution":{"iopub.status.busy":"2022-10-18T05:20:41.911942Z","iopub.execute_input":"2022-10-18T05:20:41.912349Z","iopub.status.idle":"2022-10-18T05:20:41.956436Z","shell.execute_reply.started":"2022-10-18T05:20:41.912316Z","shell.execute_reply":"2022-10-18T05:20:41.955261Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#####  **Conclusion** what ever we do, our recall score will not improving , maybe because we don't have a good amount of data, so I think if we got more data and we try more complex models our accuracy will improve","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}}]}